import os
import re
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader

data_dir = "archive"

if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"Using {torch.cuda.get_device_name()} for training")
else:
    device = torch.device("cpu")
    print("No GPU available, using CPU for training")

def preprocess_data(data_dir):
    # Load the lines and conversations data
    lines_file = os.path.join(data_dir, "movie_lines.txt")
    conv_file = os.path.join(data_dir, "movie_conversations.txt")
    lines = open(lines_file, encoding="utf-8", errors="ignore").read().split("\n")
    convs = open(conv_file, encoding="utf-8", errors="ignore").read().split("\n")
    # Create a dictionary to map line IDs to the corresponding text
    id2line = {}
    for line in lines:
        parts = line.split(" +++$+++ ")
        if len(parts) == 5:
            id2line[parts[0]] = parts[4]

    # Create a list of conversation IDs and their corresponding lines
    conversations = []
    for conv in convs:
        parts = conv.split(" +++$+++ ")
        if len(parts) == 4:
            line_ids = re.findall(r"'(.*?)'", parts[3])
            lines = [id2line[id] for id in line_ids]
            conversations.append(lines)

    # Create a list of input-output pairs for training the model
    pairs = []
    for conv in conversations:
        for i in range(len(conv)-1):
            input_line = conv[i].strip()
            output_line = conv[i+1].strip()
            if input_line and output_line:
                pairs.append((input_line, output_line))

    return pairs

def tokenize_data(pairs):
    # Create a list of all the input and output lines
    lines = [pair[0] for pair in pairs] + [pair[1] for pair in pairs]
    # Tokenize the lines using a simple whitespace tokenizer
    tokens = [line.lower().split(" ") for line in lines]

    # Create a dictionary to map tokens to indices
    token2idx = {"<PAD>": 0, "<UNK>": 1, "<SOS>": 2, "<EOS>": 3}
    for token in set([token for line in tokens for token in line]):
        if token not in token2idx:
            token2idx[token] = len(token2idx)

    # Convert the tokens to indices and pad the sequences
    input_seqs = [[token2idx.get(token, token2idx['<UNK>']) for token in line] for line, _ in pairs]
    output_seqs = [[token2idx.get(token, token2idx["<UNK>"]) for token in line] for _, line in pairs]
    input_seqs_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq).to(device) for seq in input_seqs], padding_value=token2idx["<PAD>"], batch_first=True)
    output_seqs_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq).to(device) for seq in output_seqs], padding_value=token2idx["<PAD>"], batch_first=True)

    return input_seqs_padded, output_seqs_padded, token2idx

def preprocess_data(data_dir):
    # Load the lines and conversations data
    lines_file = os.path.join(data_dir, "movie_lines.txt")
    conv_file = os.path.join(data_dir, "movie_conversations.txt")
    lines = open(lines_file, encoding="utf-8", errors="ignore").read().split("\n")
    convs = open(conv_file, encoding="utf-8", errors="ignore").read().split("\n")
    # Create a dictionary to map line IDs to the corresponding text
    id2line = {}
    for line in lines:
        parts = line.split(" +++$+++ ")
        if len(parts) == 5:
            id2line[parts[0]] = parts[4]

    # Create a list of conversation IDs and their corresponding lines
    conversations = []
    for conv in convs:
        parts = conv.split(" +++$+++ ")
        if len(parts) == 4:
            line_ids = re.findall(r"'(.*?)'", parts[3])
            lines = [id2line[id] for id in line_ids]
            conversations.append(lines)

    # Create a list of input-output pairs for training the model
    pairs = []
    for conv in conversations:
        for i in range(len(conv)-1):
            input_line = conv[i].strip()
            output_line = conv[i+1].strip()
            if input_line and output_line:
                pairs.append((input_line, output_line))

    return pairs

def tokenize_data(pairs):
    # Create a list of all the input and output lines
    lines = [pair[0] for pair in pairs] + [pair[1] for pair in pairs]
    # Tokenize the lines using a simple whitespace tokenizer
    tokens = [line.lower().split(" ") for line in lines]

    # Create a dictionary to map tokens to indices
    token2idx = {"<PAD>": 0, "<UNK>": 1, "<SOS>": 2, "<EOS>": 3}
    for token in set([token for line in tokens for token in line]):
        if token not in token2idx:
            token2idx[token] = len(token2idx)

    # Convert the tokens to indices and pad the sequences
    input_seqs = [[token2idx.get(token, token2idx['<UNK>']) for token in line] for line, _ in pairs]
    output_seqs = [[token2idx.get(token, token2idx["<UNK>"]) for token in line] for _, line in pairs]

    # Convert the lists of indices to PyTorch tensors and pad the sequences
    input_seqs_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in input_seqs], padding_value=token2idx["<PAD>"], batch_first=True).to(device)
    output_seqs_padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in output_seqs], padding_value=token2idx["<PAD>"], batch_first=True).to(device)

    # Return the padded input and output sequences and the token-to-index dictionary
    return input_seqs_padded, output_seqs_padded, token2idx

class CornellDataset(Dataset):
    def init(self, pairs, input_seqs_padded, output_seqs_padded):
        self.pairs = pairs
        self.input_seqs_padded = input_seqs_padded
        self.output_seqs_padded = output_seqs_padded
    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        input_seq = self.input_seqs_padded[idx]
        output_seq = self.output_seqs_padded[idx]
        return input_seq, output_seq

def create_data_loader(pairs, input_seqs_padded, output_seqs_padded, batch_size):
    dataset = CornellDataset(pairs, input_seqs_padded, output_seqs_padded)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    return data_loader

pairs = preprocess_data(data_dir)
input_seqs_padded, output_seqs_padded, token2idx = tokenize_data(pairs)
batch_size = 64
data_loader = create_data_loader(pairs, input_seqs_padded, output_seqs_padded, batch_size)


import torch.nn as nn

class Seq2Seq(nn.Module):
    def init(self, input_dim, emb_dim, hid_dim, output_dim, n_layers, dropout):
        super().init()
            # Define the embedding layer
        self.embedding = nn.Embedding(input_dim, emb_dim)

        # Define the encoder
        self.encoder = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)

        # Define the decoder
        self.decoder = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)
        self.fc_out = nn.Linear(hid_dim, output_dim)

def forward(self, input_seqs, output_seqs):
    # Embed the input and output sequences
    input_embedded = self.embedding(input_seqs)
    output_embedded = self.embedding(output_seqs)

    # Encode the input sequence
    encoder_output, (encoder_hidden, encoder_cell) = self.encoder(input_embedded)

    # Initialize the decoder hidden state and cell state with the encoder final hidden and cell states
    decoder_hidden = encoder_hidden
    decoder_cell = encoder_cell

    # Iterate over the output sequence and decode one token at a time
    outputs = []
    for i in range(output_embedded.size(1)):
        # Get the i-th token of the output sequence
        decoder_input = output_embedded[:, i, :]

        # Add a third dimension to the decoder input tensor to match the input shape of the decoder LSTM
        decoder_input = decoder_input.unsqueeze(1)

        # Decode the current token and get the output and hidden state of the decoder
        decoder_output, (decoder_hidden, decoder_cell) = self.decoder(decoder_input, (decoder_hidden, decoder_cell))

        # Remove the third dimension of the decoder output tensor
        decoder_output = decoder_output.squeeze(1)

        # Pass the decoder output through a linear layer and apply a softmax activation function
        output = F.softmax(self.fc_out(decoder_output), dim=1)

        # Add the output to the outputs list
        outputs.append(output)

    # Stack the outputs to create a tensor of shape (batch_size, output_seq_len, output_dim)
    outputs = torch.stack(outputs, dim=1)

    return outputs

# Define hyperparameters
batch_size = 64
num_epochs = 10
learning_rate = 0.001

# Define the PyTorch DataLoader
train_dataset = CornellMovieDialogsDataset(input_seqs_padded, output_seqs_padded)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Define the model
class EncoderDecoderModel(torch.nn.Module):
    def __init__(self, input_size, output_size, hidden_size):
        super().__init__()
        self.encoder = torch.nn.GRU(input_size, hidden_size)
        self.decoder = torch.nn.GRU(output_size, hidden_size)
        self.fc = torch.nn.Linear(hidden_size, output_size)

    def forward(self, input_seq, output_seq):
        # Encode the input sequence into a hidden state
        _, hidden_state = self.encoder(input_seq)

        # Initialize the decoder input with the <SOS> token
        decoder_input = torch.tensor([[2]], device=device)

        # Loop over the output sequence and generate predictions
        output_seq_len = output_seq.shape[1]
        predictions = []
        for i in range(output_seq_len):
            # Decode the current timestep using the previous hidden state and input
            output, hidden_state = self.decoder(decoder_input, hidden_state)

            # Project the decoder output onto the output vocabulary
            output = self.fc(output)

            # Choose the token with the highest probability as the next input
            top1 = output.argmax(2)
            predictions.append(top1)

            # Use the predicted token as the input for the next timestep
            decoder_input = top1.squeeze().detach()

        # Concatenate the predicted tokens and return as a tensor
        return torch.cat(predictions, dim=1)

# Instantiate the model and move it to the GPU
model = EncoderDecoderModel(len(token2idx), len(token2idx), hidden_size=256).to(device)

# Define the loss function and optimizer
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Train the model
for epoch in range(num_epochs):
    for batch_idx, (input_batch, output_batch) in enumerate(train_dataloader):
        # Move the input and output batches to the GPU
        input_batch = input_batch.to(device)
        output_batch = output_batch.to(device)

        # Zero the gradients and compute the forward pass
        optimizer.zero_grad()
        predictions = model(input_batch, output_batch[:, :-1])

        # Compute the loss and backpropagate the gradients
        loss = criterion(predictions.reshape(-1, len(token2idx)), output_batch[:, 1:].reshape(-1))
        loss.backward()
        optimizer.step()

        # Print the current loss
        if batch_idx % 100 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_dataloader)}, Loss: {loss.item():.4f}")

class Encoder(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(Encoder, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)

    def forward(self, input_seqs, input_lengths, hidden=None):
        embedded = self.embedding(input_seqs)
        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.to("cpu"), batch_first=True)
        outputs, hidden = self.gru(packed, hidden)
        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)
        return outputs, hidden


class Decoder(nn.Module):
    def __init__(self, output_size, hidden_size, num_layers=1):
        super(Decoder, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=2)

    def forward(self, input_seq, hidden):
        input_seq = input_seq.unsqueeze(1)
        embedded = self.embedding(input_seq)
        output, hidden = self.gru(embedded, hidden)
        output = self.softmax(self.out(output))
        return output.squeeze(1), hidden

def train(model, dataloader, optimizer, criterion, clip):
    model.train()
    total_loss = 0

    for i, batch in enumerate(dataloader):
        # Get the input and target sequences
        input_seqs = batch["input_seqs"].to(device)
        target_seqs = batch["target_seqs"].to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass through the model
        output_seqs = model(input_seqs, target_seqs)

        # Calculate the loss and perform backpropagation
        loss = criterion(output_seqs[:, 1:].reshape(-1, output_seqs.shape[2]), target_seqs[:, 1:].reshape(-1))
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()

        # Update the total loss
        total_loss += loss.item()

    return total_loss / len(dataloader)


def evaluate(model, dataloader, criterion):
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in dataloader:
            # Get the input and target sequences
            input_seqs = batch["input_seqs"].to(device)
            target_seqs = batch["target_seqs"].to(device)

            # Forward pass through the model
            output_seqs = model(input_seqs, target_seqs, teacher_forcing_ratio=0)

            # Calculate the loss
            loss = criterion(output_seqs[:, 1:].reshape(-1, output_seqs.shape[2]), target_seqs[:, 1:].reshape(-1))

            # Update the total loss
            total_loss += loss.item()

    return total_loss / len(dataloader)

# Set up the training and validation data loaders
train_dataset = CornellMovieDialogsDataset(train_pairs, input_token2idx, output_token2idx)
val_dataset = CornellMovieDialogsDataset(val_pairs, input_token2idx, output_token2idx)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

# Set up the model and optimizer
encoder = Encoder(len(input_token2idx), hidden_size, num_layers, dropout)
decoder = Decoder(len(output_token2idx), hidden_size, num_layers, dropout)
model = Seq2Seq(encoder, decoder).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Train the model
for epoch in range(num_epochs):
    train_loss = train(model, train_loader, optimizer, criterion, clip)
    val_loss = evaluate(model, val_loader, criterion)
    print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}")

# Define a function to generate a response from the model
def generate_response(model, sentence, input_token2idx, output_token2idx, max_length):
    # Tokenize the input sentence
    tokens = sentence.lower().split(" ")
    input_seq = [input_token2idx.get(token, input_token2idx['<UNK>']) for token in tokens]
    input_seq = torch.tensor(input_seq).to(device).unsqueeze(0)

    # Generate the output sequence using the model
    with torch.no_grad():
        hidden = model.encoder(input_seq)
        output_seq = model.decoder.generate(hidden, output_token2idx['<SOS>'], output_token2idx['<EOS>'], max_length)

    # Convert the output sequence to text
    output_tokens = [output_token2idx[i.item()] for i in output_seq]
    output_sentence = " ".join(output_tokens).capitalize()
    return output_sentence

# Test the model by generating a response to a sample input sentence
input_sentence = "hello"
max_length = 50
response = generate_response(model, input_sentence, input_token2idx, output_token2idx, max_length)
print(f"Input: {input_sentence}")
print(f"Response: {response}")
